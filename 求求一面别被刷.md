# (求求一面别被刷)-LRⅡ招新-数据与存储与网络爬虫
# Part1-番剧种子与金融数据爬取
## Task0-阅读资料
### 1.你对爬虫有什么自己的见解？它和我们的机器学习有什么关系？
### 2.数据和模型的关系又是什么，谁更重要？
1.爬虫就是爬取网页的各种数据吧，讲真没啥特别的见解。它为机器学习提供数据，是机器学习的必要环节
2.数据就像食材，模型就像厨师，都挺重要
不过模型似乎要更重要一些，因为即使数据存在各种问题，好的模型也能排除干扰，不过对于一个烂模型，给它再好的数据也没办法
## Task1-前备知识检验以及相关环境的配置
### 基础知识
#### 1.爬虫工具库
与爬出相关的库可以大体分为两类
一类是解决如何发起请求、获取网页内容
>如Requests

它的特点：接口简单，语法直观
适用场景：需要快速发起GET/POST请求、爬取小规模网站、做API调用
（爬网页和API调用区别举例
爬网页：抓取 HTML→解析→找到```<div class="comment">...</div>```→ 
API调用：直接访问 ```https://douban.com/api/comments?movie_id=xxx```→直接返回干净的JSON）
>如httpx

特点：支持同步和异步，兼容Requests风格
适用场景：需要高并发、异步请求的场景（爬取效率比 requests高）
***
还有一种是解析网页的
>如BeautifulSoup

特点：语法友好，支持 CSS 选择器、标签遍历
适用场景：数据规模小、网页结构不复杂
缺点：解析速度偏慢

>如lxml

特点：底层用 C 写的，解析速度快；支持 XPath
适用场景：对性能有要求，目标网站结构复杂时

>如Parsel

特点：Scrapy 内置的解析库，支持 CSS/XPath，功能比 BeautifulSoup 更面向爬虫
适用场景：Scrapy 项目或想要XPath与CSS混合解析
***
而一个爬虫框架就是一个全家桶，可以完成更更全面，更专业的相关操作
>Scrapy

特点：最成熟的爬虫框架，内置请求调度、去重、pipeline（数据清洗存储）、中间件
适用场景：中大型项目、需要爬取海量页面或复杂流程
优势：扩展性强，生态完善（如scrapy-redis分布式扩展）
（什么是分布式：
- 单机爬虫，一台电脑跑 Scrapy，把所有 URL 都抓完
- 分布式爬虫，任务队列（URL 列表）放在 共享存储里
多台机器上的爬虫进程同时从队列里取任务
每台机器各自爬取不同的网页，然后把结果存到数据库里）
>pyspider

特点：带Web界面，可在线管理任务，支持分布式
适用场景：需要可视化管理的团队项目
缺点：维护更新不活跃

>Feapder（国内开发者常用）

特点：基于Scrapy改进，支持分布式和爬虫调度管理，支持增量爬取
适用场景：对Scrapy不满足、需要大规模分布式的场合
#### 2.代理池技术
首先什么是代理？代理就是让请求不是直接从你自己的 IP 发出去，而是通过另一台服务器（代理服务器）中转，从而对外显示成“代理 IP”
那么代理池，就是把很多可用的代理 IP 集合起来，形成一个“池子”，爬虫在请求时从里面随机或轮流取一个代理来使用
为什么需要代理池？
因为目标网站的反爬机制
- IP限制：同一个 IP 在单位时间内请求太多会被封禁或者限速
地理限制：某些内容只允许特定地区的IP访问
匿名性：不想暴露自己的真实IP

使用代理池的好处：
- 绕过反爬虫：不断更换 IP，降低被封几率
- 提升并发量：多台代理同时发请求，爬取速度更快
- 访问受限内容：通过境外代理访问国外网站，或反之
***
关于如何构建和维护一个有效的代理池？
首先，代理池的核心目标
持续有代理可用：保证池子里永远有一定数量的活代理
自动化维护：不用人工手动更新，系统能自动淘汰坏代理、补充新代理
高质量优先：响应快、稳定、匿名度高的代理，要优先使用
动态分配：不同场景（高并发或者访问敏感网站）需要不同策略，比如限速、轮换
构建步骤：
1. 代理来源
- 免费代理抓取：常见网站（如「西刺代理」「快代理」「89免费代理」），用爬虫定时抓取
- 付费代理API：购买供应商的服务，直接调用 API 获取代理
- 自建代理：在不同地区买 VPS，搭建 HTTP/HTTPS/SOCKS5 代理（稳定但成本高）
最佳做法：免费代理作为补充，付费代理做核心
2. 存储设计
把代理放在一个统一的存储里，常用方案：
- Redis
优点：支持快速读写，天然支持队列和权重管理
- 数据库（MySQL/MongoDB）
优点：适合长期存储、历史分析
缺点：读写性能不如 Redis
3. 验证机制
代理池最关键的部分。验证要考虑：
- 可用性：能否访问目标网站
- 匿名性：是否暴露真实 IP
- 响应速度：访问速度是否在合理范围
- 稳定性：是否能连续多次成功
实现方式：
定时任务不断检测池子里的代理
检测方法：用代理访问目标网站（或通用测试网站如 httpbin.org/ip）
给每个代理打分：
成功 +1
失败 -1
分数降到0→淘汰
4. 动态维护
- 代理补充：当池子里代理数量 < 阈值（比如 100），就-启动爬虫或 API 拉取新代理
- 代理淘汰：长时间失败或响应过慢的代理剔除
- 负载均衡：
随机取代理（防止单个代理过载）
根据权重取代理（优先快且稳定的）

#### 3.并发编程
##### 线程和进程的区别
- 定义层面
进程：操作系统中正在运行的一个程序，是资源分配的最小单位，拥有独立的内存空间

- 资源开销
进程：切换时开销大
线程：只需切换寄存器、栈，开销小，上下文切换比进程轻量

- 内存与数据共享
进程间：相互独立，通信需要IPC（管道、消息队列、共享内存、socket）
线程间：共享进程内的内存，通信方便，但容易产生线程安全问题（需要锁、队列控制）
##### 用多线程不用多进程
GIL指任意时刻，只允许一个线程执行Python字节码
但GIL在IO操作时会释放，让其他线程有机会运行
所以IO密集任务，多线程能显著提高吞吐量
具体来讲就是：
- 共享内存：所有线程都在同一进程中，天然共享URL队列、代理池、缓存数据
- 上下文切换开销小：线程切换比进程切换快
- 利用IO等待空档：线程在等待网络响应时，GIL 释放，其他线程可以继续爬取
所以
爬虫发起请求→线程 A 等待服务器响应
操作系统把CPU时间片切换到线程B→线程B继续请求或解析数据
整体吞吐量显著提升，而不会因为GIL被卡死
##### 具体实现
1. 准备任务队列
里面放一堆待爬取的URL，可以用queue.Queue
2. 定义任务函数
每个线程执行的工作：从队列取出URL → 发送请求 → 解析结果→存储数据
3. 启动线程池
如concurrent.futures.ThreadPoolExecutor方法
同时跑多个线程，每个线程去执行任务函数
4. 等待所有线程结束
确保所有URL都被爬完

#### 4.性能指标
##### 什么是RPS？
RPS，每秒发出的请求数，是衡量爬虫性能和并发度的关键指标
如果爬虫在 10 秒内成功抓取了 200 个页面 → RPS = 200 ÷ 10 = 20
##### 如何测量 RPS？
方法一：简单统计
在代码里记录开始和结束时间 + 请求数
方法二：用日志+移动窗口
在多线程/协程爬虫里，可以每隔 1 秒统计一次请求数，打印出来
Scrapy就有类似的统计（logstats 扩展会打印 RPS）
方法三：监控工具
大规模分布式爬虫，可以接入Prometheus+Grafana，实时绘制RPS曲线

##### 如何优化 RPS？
(1) 提高并发度
多线程 / 协程 / 分布式
(2) 降低延迟
启用持久连接，减少握手开销
用更快的HTTP客户端（如 httpx、aiohttp）。
(3) 优化代理池
多个代理IP并发发请求，避免单IP被限速。
(4) 控制爬取策略
如果目标站点有限速策略，可以调节sleep或rate-limit，保持在安全阈值
有时候“更高 RPS”并不意味着“更好”，要平衡效率和封禁风险
(5) 分布式架构
使用多台机器+分布式队列，整体RPS可以线性扩展

#### 5.反爬对策
##### 常见的网站反爬虫措施
针对这些措施有哪些应对策略？
1. 基础层面
User-Agen 检测：拦截不带 UA 或使用默认 UA 的请求
Referer 检测：检查请求来源是否来自站内页面
Robots.txt：约束爬虫访问范围
1. 流量限制
频率限制：短时间内访问过快）
IP 封禁：检测到异常流量后，封禁IP
并发限制：同一用户/IP 的并发连接数超过阈值会被限制
1. 技术手段
Cookie / Session 校验：请求需要绑定特定的会话
登录验证：部分内容仅对登录用户开放
动态请求参数：请求URL或POST参数中包含加密/时间戳/签名
验证码：访问频繁或关键接口时触发滑块 / 点选 / 图形验证码
1. 行为分析
鼠标/键盘轨迹检测：判断访问是否模拟真实用户操作。
浏览器指纹：收集屏幕分辨率、字体、插件、Canvas 指纹等，识别爬虫。
反自动化框架检测：检测 Selenium、PlaywrightPuppeteer等的特征

##### 常见应对策略
1. 基础伪装
设置随机User-Agent
设置Referer、Accept-Language、Cookies 来模拟浏览器请求
1. 控制流量
降低请求频率：加延时、随机sleep，避免过于规律
使用代理池：IP 轮换，避免单一 IP 被封禁
分布式爬虫：多机器并发，但要注意分散流量
1. 技术破解
会话保持：使用requests.Session或Scrapy的cookie middleware
模拟登录：抓包分析登录请求，或用浏览器自动化完成登录获取 Cookie
破解验证码：
- 简单验证码：OCR（如 Tesseract、ddddocr）
- 滑块验证码：轨迹模拟 + 图像缺口识别
- 点选验证码：深度学习识别

#### 6.HTTP协议
##### 原理
HTTP，全称超文本传输协议，是用于从万维网服务器传输超文本到本地浏览器的传送协议，是一种应用层协议，是基于TCP/IP 通信协议来传递数据的
当我们在浏览器输入一个网址，此时浏览器就会给对应的服务器发送一个HTTP请求，对应的服务器收到这个请求之后，经过计算处理，就会返回一个HTTP响应
- 协议：为了使数据在网络上从源头到达目的，网络通信的参与方必须遵循相同的规则，这套规则称为协议，它最终体现为在网络上传输的数据包的格式
- 万维网：是一个基于互联网、利用超文本技术，将全球的信息资源用网页形式互相连接、并通过浏览器访问的系统
##### 完整的http请求
![alt text](<img/截屏2025-09-30 11.14.53.png>)
##### 请求头
![alt text](<img/截屏2025-09-27 20.32.29.png>)

#### 7.请求方式
![alt text](<img/截屏2025-09-30 11.19.04.png>) 
![alt text](<img/截屏2025-09-30 11.19.44.png>)

### scrapy
#### 核心组件
感觉教程讲得很清楚了，就直接贴图吧
![alt text](<img/截屏2025-09-30 11.41.45.png>)
#### 工作流程
Spider生成初始Request请求
引擎接收Request然后发给调度器
请求入队调度器
引擎从调度器取出请求交给下载器
下载器发送HTTP请求然后获取Response响应
下载器把响应返回给引擎
引擎把响应交给Spider处理
Spider解析Response，产生Item和新的Request
Item交给引擎再交给通道
通道处理Item，进一步数据处理，在存储
当调度器中不存在任何请求了，整个程序才会停止
#### 实战应用
在settings.py中定义多个自定义Pipeline类，并在setting中给定优先级
每个Pipeline的process_item方法负责一种处理与存储方式

### 环境配置
![alt text](<img/截屏2025-09-30 11.52.40.png>)
 ![alt text](<img/截屏2025-09-30 11.53.10.png>)

## Task2-Mikan计划的爬取
### 一.截图
![alt text](<img/截屏2025-09-30 23.24.38.png>)
![alt text](<img/截屏2025-09-30 23.25.02.png>) 
![alt text](<img/截屏2025-09-30 23.25.24.png>)
### 二.我的学习笔记
>学长好，这里是我根据你所谓给的scrapy视频教程，做的笔记。
因为教程和该任务的具体的实现肯定是有所不一样的
我先完整地呈现我跟着视频教程所完成的笔记与实现
再在这个基础上，补充我对该任务Mikan爬取的过程与实现
#### 1.创建一个爬虫项目
##### 1.1安装scrapy软件包
![alt text](<img/截屏2025-09-29 14.41.02.png>)
```pip config set global.index-url https://pypi.doubanio.com/simple```
使用豆瓣镜像源，下载速度快，而且进行的是global全局设置，所以以后都不用再重复设置
```pip install scrapy```，安装scrapy框架
##### 1.2查看指令
直接输入scrapy来查看指令
![alt text](<img/截屏2025-09-29 14.47.26.png>)
##### 1.3切到项目指定位置

```cd /Users/DX3906/Desktop/工作室/凌睿/二轮/爬虫```
##### 1.4新建项目文件
![alt text](<img/截屏2025-09-29 14.49.23.png>)
```scrapy startproject spider3906```
生成的一个项目文件夹里面有一个同名文件夹，里面有scratch框架会使用到的一些文件，其中一个文件夹叫spiders。这就是将来我们写的蜘蛛程序的地方。
我们的重点是在解析页面，而解析页面的代码就要写到这个spiders里面

##### 1.5进入项目目录
```cd spider3906```
##### 1.6创建一个爬虫
```scrapy genspider douban movie.douban.com```
创建一个爬虫，指定名字"douban"和爬取域的范围"scrapy genspider douban movie.douban.com"
>注意，这里只写域名，不写https：
#### 2.pycharm配置环境
![alt text](<img/截屏2025-09-24 21.07.21.png>)
用pycham去打开这个项目文件夹然后加虚拟环境
在项目路径上建立虚拟环境
然后安装scrapy软件包
##### 小小疑问 
>怎么又要装scrapy？

刚才那个scrapy是装在全局环境，创建项目的
现在这个是装在虚拟环境里的
#### 3.一页爬取
##### 3.1 CSS解析
现在打开spiders文件夹，里面的“douban.py”就是我们刚刚创建的爬虫，现在来完善它
首先为它设定第一个需要爬取的第一个url：movie.douban.com/top250
向服务器发请求,然后拿到数据返回响应，靠parse函数解析返回的网页数据，提取结构化数据(生成item)
响应首先把响应包装成一个叫select的对象
有了这个select对象，即选择器对象，它里面有CSS方法，支持CSS选择器解析
xpath方法，支持xpath语法解析
re方法，支持正则表达式解析
所以接下来选择我们的解析页面的方式
比如就用CSS解析
进行网页检查，发现我们需要的东西是“ol（）”下的一个个列表，然后我们复制一个选择器
![alt text](<img/截屏2025-09-24 23.45.37.png>)

去掉后面的```:nth-child(1)```，因为我们要取的是所有li，而不只是其中一个
然后采用css方法解析,css方法返回的仍然是选择器对象
然后对这些列表做循环，每一个列表项又是一个选择器对象
然后再根据公式用css方法去取里面的内容
返回的仍然是选择器对象s
但这次我们想要的具体的数据
所以用extract_first（）抽取里面的第一条数据
![alt text](<img/截屏2025-09-25 09.21.54.png>)
##### 3.2自定义item类
这个数据需要把它组装一下，需要组装成一个对象，也就是scraping框架中的item对象
所以制定一个类去继承这个item类这个父类
这个自定义的MovieItem类的一个movie_item对象就代表一部电影的数据
![alt text](<img/截屏2025-09-25 13.44.53.png>)
现在，在自定义类中申明我们要提取的信息的字段
如name = scrapy.Field() 表示声明了一个名为name的字段

##### 3.3 item对象的赋值
然后在douban.py中导入这个类
```from spider3906.items import MovieItem```
![alt text](<img/截屏2025-09-25 13.44.39.png>)
接着 实例化一个MovieItem对象
将数据组装到对象中
赋值时既可以用属性访问“.”又可以用键值对“[]”的方法
不过最好用键值对[]的方法
因为
1. Item 本质上是对字典的封装（继承自 dict）使用 [] 操作更符合Python字典的操作习惯
2. 如果定义的字段名与Item类的内置方法或属性重名（如 get、items 等），使用 . 语法会导致冲突，而 [] 语法可以避免这个问题
最后也就会生成25个item对象，不能直接return
而是用生成器的方式来产出这个数据
yield

##### 3.4 神奇的生成器
>参考的教程[Scrapy 入门教程](https://www.runoob.com/markdown/md-link.html)

要使函数的结果的复用性高，如果函数的”结果“不止一个，就得用列表把结果保存
但是这样太占内存啦
所以就有了生成器这个概念
每次循环都会执行函数内部的代码，执行到yield时，fab 函数就返回一个迭代值，下次迭代时，代码从 yield的下一条语句继续执行
就好像看起来就好像一个函数在正常执行的过程中被 yield中断了数次，每次中断都会通过yield返回当前的迭代值
这使得我们能够自由地取函数中的我们想要的结果，而无需将结果全都保留起来
##### 3.5 更改请求器
```USER_AGENT = 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6)AppleWebKit/537.36(KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36'```
然后需要改“请求头”，在配置setting文件里面，我们改一下我们的user agent伪装成自己是个浏览器
##### 3.6保存数据
```scrapy crawl douban -o douban.csv```
将刚才parse函数提取到的信息保存到当前目录下的 douban.csv文件
#### 4.多页爬取（方法一）
##### 4.1获取UIL
commend+F在源代码搜索“paginator”
![alt text](<img/截屏2025-09-25 13.40.28.png>)
拿到这里的超链接，也就是这个a标签的HREF属性就可以拿到一个新的URL，然后就可以拿到新页面了
右键复制选择器
然后修改为```"div.paginator > a::attr(href)"```
其中的```a::attr(href)"```，表示拿A标签的HREF属性
但是拿到的这个地址并不完整
需要专门的response对象的url的拼接方法
然后就是一个完整的URL
我们来打印一下，如题所示
![alt text](<img/截屏2025-09-25 13.51.42.png>)
##### 4.2 返回请求
现在需要把这个URL构造成一个叫request对象（交给引擎，再由引擎交给调度器）
要返回一个请求对象
但就像前面说的，不能用return，因为这是一个生成器
#### 5.多页爬取（方法二）
不过想方法一这样子做有一个bug！🙀
![alt text](<img/截屏2025-09-25 14.06.59.png>)
这里显示有300多个数据
减去刚刚的25个数据，还多了25个数据
>这是为什么呢？

原来是取url的时候，获取的是所有页面的url
不过启始页的我们已经爬过了，按理说调度器不会爬重复的网页，不过因为起始页两次的url形式不一样
一个是```https://movie.douban.com/top250?start=0&filter=```

一个是```https://movie.douban.com/top250）”```
调度器认为它们不一样，所以就重复爬取了
当然可以直接把 start_urls换成```https://movie.douban.com/top250?start=0&filter=```
不过有更好的方法
就是开始的时候我们不用tart_urls，取而代之用一个方法叫```start_requests```
不是要爬十个页面吗？那就构造好十个请求
（需要注意的是要导入scrapy框架下的request，而不是其他的request）
然后构造对应的url



#### 6.补充
##### 6.1 关于导入
先导官方的，再导第三方库，最后再导自己写的
然后按照字母表顺序
可以使用code中的优化imort进行自动调整
##### 6.2无伤大雅的警告⚠️
有一个小警告
![alt text](<img/截屏2025-09-29 20.10.00.png>)
因为我们写的parse函数是对父类parse函数的重写
所以尽量参数啥的保持一致，不过也没啥影响
##### 6.3 requirements.txt
如果想看一看已经安装了哪些依赖项，哪些三方库，用什么命令？有两个命令可以做到这一点
第一个命令```pip list```
还有一个命令```pip freeze```
于是就会在终端输出依赖项清单
而通常项目里面会放一个依赖项清单文件
因为当别人这个项目的时候，他可以照着这个依赖项清单，自己重建虚拟环境，重建依赖项
这就要用到```pip freeze > requirements.txt```
“>”叫做输出重定向,表示把输出的内容不要不要输出到终端里面，而是放到指定的文件中

安装文件里的依赖项用```pip install -r requirements.txt```

#### 7.导入excel表格
##### 7.1创建工作簿，工作表
![alt text](<img/截屏2025-09-29 20.13.08.png>)
先pip安装openpyxl 
写数据和处理数据要找数据管道,也就是piplines文件
```self.wb = openpyxl.Workbook()```创建一个工作簿对象
可以拿到它里面自带的默认的有一张工作表了
或者可以用```self.wb.create_sheet()```创建一张新的工作表
然后工作表改个名字，再加上表头
然后以后就可以往这个工作表里面写数据了
##### 7.2保存
保存工作簿
等把数据全部爬完的时候，当爬虫停止运行的时候保存，用一个方法叫close_spider
```self.wb.save("电影数据.xlsx")```数据就保存在电影数据.xlsx文件
##### 7.3钩子函数
像close_spider这样的方法（以及process_item，__ init __（））叫做钩子方法或者钩子函数，你不需要去调它而是scrapy框架会主动来调它
##### 7.4数据处理
process_item（）看名字就知道处理数据的
从item里面去拿电影的标题，从item里面拿电影的评分rank
然后添加到ws中，也就是excel的表中

拿数据最好不要用下面👇的键值的这种方式去拿因为如果这个数据不存在就会报错
```self.ws.append((item["title"],item["rank"]))```
要如下题提前把这个数据稍微预处理一下
![alt text](<img/截屏2025-09-29 20.20.29.png>)
用字典拿数据的一种方法get方法
图上的两种get方法都可以
##### 7.5改配置
然后需要改一下配置，把这个管道配置好了，管道才能生效
数字小的先执行，数字大的后执行
但我们现在只有一个管道，无需管她
>如图所示，成功！！！
![alt text](<截屏2025-09-29 20.22.58.png>)
#### 8.导入数据库

##### 8.1关于什么是端口，IP地址，端口
搞数据库之前，这些是很有必要搞懂的
我直接问的AI，我觉得她的回答已经很到位了
所见即所得，似乎不再需要我总结什么
我就直接贴图吧
![alt text](<img/截屏2025-09-28 16.21.05.png>)
![alt text](<img/截屏2025-09-28 16.21.21.png>) 
![alt text](<img/截屏2025-09-28 16.21.37.png>)

##### 8.2 连接到MySQL服务器与创建用户
mysqlworkbench主页里的mysql connection和python中的.conn = pymysql.connect（）干的其实就是同一件事
也就是
建立到MySQL服务器的连接
（当然，一个是可视化操作、一个手动执行SQL）
而这个服务器运行在我的电脑上（127.0.0.1是我电脑IP地址），3306是它监听的端口

刚才说的建立连接，是用户与服务器连接
而默认有一个root超级用户，它拥有全部的权限
现在可以建立其他的用户，并指定的她的权限，与访问的Ip
如果Limit to Hosts Matching填的是默认的“%”，那这个用户可以从任何地方连接到我们的服务器，如果指定特定的Ip地址，那她也就只能从这个IP地址连接上我们的服务器
![alt text](<img/截屏2025-09-28 16.39.17.png>) 
![alt text](<img/截屏2025-09-28 16.39.20.png>)
然后我们呢，要在服务器上创建一个数据库
如图所示，创建一个叫spider的数据库
![alt text](<img/截屏2025-09-28 16.56.30.png>)
##### 8.3 在python中进行连接
现在我们在python中进行连接，用刚刚创建的那个“Luo”的用户，如图所示，也就建立了连接
![alt text](<img/截屏2025-09-28 16.59.20.png>)
##### 8.4创建游标
```self.cursor = self.conn.cursor()```
创建一个游标对象
这个游标就是执行SQL的“工具人”：用它来执行SQL、获取结果
##### 8.5用游标执行SQL
![alt text](<img/截屏2025-09-29 21.27.52.png>)
##### 8.6 提交，关闭连接
![alt text](<img/截屏2025-09-29 21.31.53.png>)

### 三.Mikan的爬取
#### 信息提取
我们现在已经有了一个选择器
这个Mikan的HTML结构和豆瓣的HTML的结构似乎有很大差别
为了提取其中信息不得不进行HTML的相关知识的学习
#### HTML结构
在HTML里，页面是由很多标签组成的，显然标签中存在着所属关系
![alt text](<img/截屏2025-09-29 18.51.12.png>)
比如我现在要提取图中的标题
![alt text](<img/截屏2025-09-29 18.48.09.png>)
就用
```td:nth-child(3) a.magnet-link-wrap::text```
- ```td:nth-child(3)```表示选中当前 < tr > 标签里面的第三个< td >标签
- ```a```表示这个< td >标签里的< a >标签。
- ```::text```表示提取< a >里的文本内容
- ```.magnet-link-wrap```表示的是带有magnet-link-wrap这个class属性的a标签，因为同一个< td >下可能有很多个a标签
***
然后有一些内容不是标签里的文本内容，而就是它的属性，比如下图
![alt text](<img/截屏2025-09-29 19.25.57.png>)
那么这个时候，就用```::attr(属性名)```
![alt text](<img/截屏2025-09-29 19.45.01.png>)
>单页面爬取成功！！！
![alt text](<img/截屏2025-09-29 19.42.08.png>)
#### 多页面爬取
采用同爬取豆瓣网的多页面方法二
![alt text](<img/截屏2025-09-29 20.06.04.png>)
>成功！！！
![alt text](<img/截屏2025-09-29 20.05.28.png>)

不过这里需要注意⚠️的是，复制地址的时候可能有“空格”，粘贴后一定要把空格给删掉，不然会报错，我第一次就在到了这个坑里了

#### FilesPipeline，启用！
不过现在不完全是题目要求的意思
题目想让我们，把.torrent文件下载到本地
这个时候就要启动FilesPipeline
FilesPipeline是Scrapy自带的文件下载管道，不过需要注意的是，它只会处理那些包含 file_urls字段的item
而且爬到的也不是完整的下载地址，所以需要urljoin()一下
下载后把文件存入指定的FILES_STORE目录
然后自动生成的files字段中就会有torrent文件的相对地址
不过题目要求的好像是绝路径
所以呢我的做法就是取出files中的path然后再加上downloads的地址，然后这就是绝对路径了，不过第一次貌似加反了
![alt text](<img/截屏2025-09-30 22.30.51.png>)

#### 字幕组信息
刚刚字幕组信息的爬取其实是出了一点问题的，因为字幕组的信息在htlm中的格式有两种，所以用了一个if not
![alt text](<img/截屏2025-09-30 21.55.22.png>)


#### 选择保存
有些字段其实我们是不想要的
可以在setting中设置
```FEED_EXPORT_FIELDS = ["title", "size", "magnet", "transform","fullpath"]```

#### RPS
![alt text](<img/ss截屏2025-09-30 23.39.14.png>)
每次调用parse()的时候就有了一个新请求
最后在爬虫结束后就计算时间和总请求数再相除
